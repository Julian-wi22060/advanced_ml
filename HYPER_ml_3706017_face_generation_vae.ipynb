{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Variational Autoencoder (VAE) - Gesichtsgenerierung mit CelebA Dataset\n",
    "\n",
    "In diesem Notebook wird ein Variational Autoencoder (VAE) auf dem CelebA-Datensatz trainiert, um realistische Gesichtsbilder zu generieren.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "35118d5dc137871b"
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [
    {
     "data": {
      "text/plain": "<torch._C.Generator at 0x1057633f0>"
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "import logging\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import kagglehub\n",
    "\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-20T16:28:36.130590Z",
     "start_time": "2025-05-20T16:28:36.096984Z"
    }
   },
   "id": "1a70d274f2bbf524"
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [],
   "source": [
    "# Set up logging to file\n",
    "os.makedirs(\"data/logs\", exist_ok=True)\n",
    "log_file = os.path.join(\"data/logs\", \"training.log\")\n",
    "logging.basicConfig(\n",
    "    filename=log_file,\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    ")\n",
    "logger = logging.getLogger()\n",
    "\n",
    "# Check for GPU support\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    logger.info(\"Using CUDA (NVIDIA GPU)!\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    logger.info(\"Using MPS (Apple GPU)!\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    logger.info(\"Using CPU!\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-20T16:28:36.137491Z",
     "start_time": "2025-05-20T16:28:36.109460Z"
    }
   },
   "id": "df026e54861573d6"
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [],
   "source": [
    "# Download CelebA dataset via Kagglehub\n",
    "data_path = kagglehub.dataset_download(\"jessicali9530/celeba-dataset\")\n",
    "logger.info(f\"Dataset path: {data_path}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-20T16:28:36.510029Z",
     "start_time": "2025-05-20T16:28:36.113347Z"
    }
   },
   "id": "ce8b41f0a7e079b8"
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [],
   "source": [
    "# Define image transformations\n",
    "image_size = 64\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((image_size, image_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5),\n",
    "                         (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# Load dataset\n",
    "dataset = datasets.ImageFolder(root=data_path, transform=transform)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-20T16:28:38.211048Z",
     "start_time": "2025-05-20T16:28:36.514027Z"
    }
   },
   "id": "8da66de5544a14ed"
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [],
   "source": [
    "# Define VAE model class\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, latent_dim=100):\n",
    "        super(VAE, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "        self.fc_mu = nn.Linear(256 * 8 * 8, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(256 * 8 * 8, latent_dim)\n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 256 * 8 * 8),\n",
    "            nn.ReLU(),\n",
    "            nn.Unflatten(1, (256, 8, 8)),\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 3, kernel_size=4, stride=2, padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        mu = self.fc_mu(x)\n",
    "        logvar = self.fc_logvar(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decoder(z), mu, logvar"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-20T16:28:38.218653Z",
     "start_time": "2025-05-20T16:28:38.217273Z"
    }
   },
   "id": "e44eff3e06fd6970"
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [],
   "source": [
    "# Define loss function\n",
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    # Reconstruction loss (MSE)\n",
    "    reconstruction_loss = nn.functional.mse_loss(recon_x, x, reduction='sum')\n",
    "    # KL divergence\n",
    "    kl_divergence = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return reconstruction_loss + kl_divergence\n",
    "\n",
    "# Random search configuration\n",
    "search_space = {\n",
    "    'latent_dim': (64, 512),\n",
    "    'learning_rate': (1e-4, 3e-3),\n",
    "    'batch_size': (32, 256)\n",
    "}\n",
    "num_trials = 5\n",
    "results = []"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-20T16:28:38.222762Z",
     "start_time": "2025-05-20T16:28:38.220663Z"
    }
   },
   "id": "8667e5369d181f0d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for trial in range(1, num_trials + 1):\n",
    "    # --- Sample hyperparameters ---\n",
    "    # latent_dim: uniform int in [64,512]\n",
    "    min_ld, max_ld = search_space['latent_dim']\n",
    "    latent_dim = random.randint(min_ld, max_ld)\n",
    "\n",
    "    # learning_rate: log-uniform in [1e-4,3e-3]\n",
    "    lr_min, lr_max = search_space['learning_rate']\n",
    "    log_low, log_high = math.log10(lr_min), math.log10(lr_max)\n",
    "    lr = 10 ** random.uniform(log_low, log_high)\n",
    "\n",
    "    # batch_size: uniform int in [32,256]\n",
    "    min_bs, max_bs = search_space['batch_size']\n",
    "    batch_size = random.randint(min_bs, max_bs)\n",
    "\n",
    "    logger.info(\n",
    "        f\"Trial {trial}/{num_trials} - \"\n",
    "        f\"latent_dim={latent_dim}, lr={lr:.2e}, batch_size={batch_size}\"\n",
    "    )\n",
    "\n",
    "    # --- Prepare DataLoader & Model ---\n",
    "    loader    = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    vae       = VAE(latent_dim=latent_dim).to(device)\n",
    "    optimizer = optim.Adam(vae.parameters(), lr=lr)\n",
    "\n",
    "    # --- Training with Early Stopping ---\n",
    "    best_loss = float('inf')\n",
    "    patience, patience_counter = 3, 0\n",
    "    eval_interval = 100\n",
    "    step, total_loss = 0, 0\n",
    "\n",
    "    vae.train()\n",
    "    while patience_counter < patience:\n",
    "        for data, _ in loader:\n",
    "            data = data.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            recon_batch, mu, logvar = vae(data)\n",
    "            loss = loss_function(recon_batch, data, mu, logvar)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            step += 1\n",
    "\n",
    "            if step % eval_interval == 0:\n",
    "                avg_loss = total_loss / eval_interval\n",
    "                logger.info(f\"Trial {trial} - Step {step} - Avg Loss: {avg_loss:.4f}\")\n",
    "                if avg_loss < best_loss:\n",
    "                    best_loss = avg_loss\n",
    "                    patience_counter = 0\n",
    "                    os.makedirs(\"data/model_checkpoints\", exist_ok=True)\n",
    "                    ckpt = f\"data/model_checkpoints/vae_trial{trial}.pt\"\n",
    "                    torch.save(vae.state_dict(), ckpt)\n",
    "                    logger.info(f\"New best model saved: {ckpt}\")\n",
    "                else:\n",
    "                    patience_counter += 1\n",
    "                    logger.warning(f\"No improvement. Patience {patience_counter}/{patience}\")\n",
    "                total_loss = 0\n",
    "\n",
    "            if patience_counter >= patience:\n",
    "                break\n",
    "        if patience_counter >= patience:\n",
    "            break\n",
    "\n",
    "    logger.info(f\"Trial {trial} completed. Best avg loss: {best_loss:.4f}\")\n",
    "    results.append((latent_dim, lr, batch_size, best_loss))\n",
    "\n",
    "# Nach allen Trials: bestes Ergebnis\n",
    "best = min(results, key=lambda x: x[3])\n",
    "logger.info(f\"Best config: latent_dim={best[0]}, lr={best[1]:.2e}, batch_size={best[2]}, loss={best[3]:.4f}\")"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2025-05-20T16:28:38.228527Z"
    }
   },
   "id": "dac0fa33a56a915f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# results is a list of tuples: (latent_dim, lr, batch_size, loss)\n",
    "best_trial = min(results, key=lambda x: x[3])                   # pick the tuple with smallest loss\n",
    "latent_dim, lr, batch_size, best_loss = best_trial             # unpack it\n",
    "logger.info(\n",
    "    f\"Random search completed. Best config: \"\n",
    "    f\"latent_dim={latent_dim}, lr={lr}, batch_size={batch_size}, loss={best_loss:.4f}\"\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "234b4a4142d3fcdd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load the best model file corresponding to the tuple index\n",
    "idx = results.index(best_trial) + 1  # trial number\n",
    "print(f\"Loading best model for latent_dim={latent_dim}, lr={lr}, batch_size={batch_size}\")\n",
    "vae = VAE(latent_dim=latent_dim).to(device)\n",
    "vae.load_state_dict(torch.load(f\"data/model_checkpoints/vae_trial{idx}.pt\", map_location=device))\n",
    "vae.eval()\n",
    "\n",
    "# Visualization loader\n",
    "dataloader_vis = DataLoader(dataset, batch_size=512, shuffle=True)\n",
    "latent_vectors, image_samples = [], []\n",
    "for i, (data, _) in enumerate(dataloader_vis):\n",
    "    data = data.to(device)\n",
    "    with torch.no_grad():\n",
    "        _, mu, _ = vae(data)\n",
    "        latent_vectors.append(mu.cpu().numpy())\n",
    "        image_samples.append(data.cpu().numpy())\n",
    "    if len(latent_vectors) * 512 >= 5000:\n",
    "        break\n",
    "latent_vectors = np.concatenate(latent_vectors, axis=0)\n",
    "image_samples = np.concatenate(image_samples, axis=0)\n",
    "print(f\"Collected latent vectors shape: {latent_vectors.shape}\")"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "87ab3e374da2a53"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Dimensionality reduction to 2D with t-SNE\n",
    "print(\"ðŸ”„ Reduziere die Dimensionen fÃ¼r Visualisierung...\")\n",
    "tsne = TSNE(n_components=2, random_state=42)      # initialize t-SNE\n",
    "latent_2d = tsne.fit_transform(latent_vectors)     # fit & transform\n",
    "\n",
    "# Plot the 2D latent space\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(latent_2d[:, 0], latent_2d[:, 1], s=5, alpha=0.6)  # color will default\n",
    "plt.title(\"Visualisierung des latenten Raumes mit t-SNE\")\n",
    "plt.xlabel(\"Dimension 1\")\n",
    "plt.ylabel(\"Dimension 2\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "c52909492483e8cf"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "z = torch.randn(16, latent_dim).to(device)\n",
    "generated_images = vae.decoder(z)\n",
    "\n",
    "# RÃ¼cktransformation: von [-1,1] zurÃ¼ck zu [0,1]\n",
    "gen_imgs = (generated_images * 0.5) + 0.5  \n",
    "\n",
    "# Erstelle ein 4Ã—4-Grid\n",
    "fig, axes = plt.subplots(4, 4, figsize=(8, 8))\n",
    "for idx, ax in enumerate(axes.flat):\n",
    "    img = gen_imgs[idx]\n",
    "    ax.imshow(np.transpose(img, (1, 2, 0)))  # (C,H,W) â†’ (H,W,C)\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "3b195660ec786d8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "422f04a59013ecee"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
